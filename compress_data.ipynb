{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook For Compressing Healthcare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, StructType, StructField, FloatType, IntegerType\n",
    "import numpy as np\n",
    "from rapidfuzz import fuzz\n",
    "import os  # import os module\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .appName(\"Compress Healthcare Data\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Helper Functions ################\n",
    "\n",
    "def read_all_files_in_data_dir(directory):\n",
    "    \"\"\"\n",
    "    Read in all datasets in a directory and read them into a pyspark dataframe\n",
    "    \n",
    "    :return dict -> key:(filename): value (pyspark_data storing data)\n",
    "    \"\"\"\n",
    "    dataframes_list = {}\n",
    "    for entry in os.scandir(directory):  \n",
    "        if entry.is_file():  # check if it's a file\n",
    "            file_path = entry.path\n",
    "            if file_path in ['data/.DS_Store', 'data/data.zip']:\n",
    "                continue\n",
    "            # Read File into pyspark dataframe\n",
    "            name_of_file = entry.path.split('/')[1].split('.')[0]\n",
    "            print(f\"Reading File Path: {file_path}\")\n",
    "            df = spark.read.csv(file_path, header=True)\n",
    "            dataframes_list[name_of_file] = df\n",
    "    print(\"SUCCESS: All Data in Data Directory Read\")\n",
    "    return dataframes_list\n",
    "\n",
    "def print_schema_for_all_dataframes(dataframes_list):\n",
    "    \"\"\"\n",
    "    Print Schema For All Dataframes in dataframe list and write out to file\n",
    "    \"\"\"\n",
    "    with open('all_data_schemas.txt', 'w') as file:\n",
    "        for df_key in dataframes_list.keys():\n",
    "            df = dataframes_list[df_key]\n",
    "            schema_string = df._jdf.schema().treeString()\n",
    "            file.write(f\"Schema for file: {df_key}\\n\")\n",
    "            file.write(schema_string)\n",
    "            file.write('\\n----------------------------\\n')\n",
    "\n",
    "def convert_add_pyspark_df_to_pandas(dataframes_list):\n",
    "    \"\"\"\n",
    "    Take list of pyspark dataframes, convert to pandas dataframes, convert to pickle file, and write to pandas_dataframes directory\n",
    "    \"\"\"\n",
    "    for df_key in dataframes_list.keys():\n",
    "        dataframes_list[df_key].write.mode('overwrite').parquet(f'parquet_files/{df_key}')\n",
    "        # pandas_df = dataframes_list[df_key].toPandas()\n",
    "        # pandas_df.to_pickle(f'pandas_dataframes/{df_key}.pkl')\n",
    "    print(\"SUCCESS: All dataframes have been written as parquet files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list = read_all_files_in_data_dir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_schema_for_all_dataframes(dataframes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_add_pyspark_df_to_pandas(dataframes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on how to read the data\n",
    "\n",
    "# import pandas as pd\n",
    "# import pyarrow\n",
    "\n",
    "# df = pd.read_parquet('parquet_files/d_hcpcs/')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
