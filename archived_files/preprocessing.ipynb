{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['SPARK_HOME'] = 'C:/Users/John/AppData/Local/Programs/Python/Python312/Lib/site-packages/pyspark'  # Update this path to your Spark installation\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .appName(\"Compress Healthcare Data\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_files_in_data_dir(directory):\n",
    "    \"\"\"\n",
    "    Read in all .csv datasets in a directory and read them into a pyspark dataframe\n",
    "    \n",
    "    :return dict -> key:(filename): value (pyspark_data storing data)\n",
    "    \"\"\"\n",
    "    dataframes_list = {}\n",
    "    for entry in os.scandir(directory):  \n",
    "        if entry.is_file() and entry.path.endswith('.csv'):  # check if it's a .csv file\n",
    "            file_path = entry.path\n",
    "            # Read File into pyspark dataframe\n",
    "            name_of_file = entry.path.split('/')[-1].split('.')[0]\n",
    "            print(f\"Reading File Path: {file_path}\")\n",
    "            df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "            dataframes_list[name_of_file] = df\n",
    "    print(\"SUCCESS: All .csv Data in Data Directory Read\")\n",
    "    return dataframes_list\n",
    "\n",
    "def print_schema_for_all_dataframes(dataframes_list):\n",
    "    \"\"\"\n",
    "    Print Schema For All Dataframes in dataframe list and write out to file, including row counts.\n",
    "    \"\"\"\n",
    "    with open('all_data_schemas.txt', 'w') as file:\n",
    "        for df_key in dataframes_list.keys():\n",
    "            df = dataframes_list[df_key]\n",
    "            schema_string = df._jdf.schema().treeString()\n",
    "            row_count = df.count()\n",
    "            file.write(f\"Schema for file: {df_key}\\n\")\n",
    "            file.write(f\"Row count: {row_count}\\n\")\n",
    "            file.write(schema_string)\n",
    "            file.write('\\n----------------------------\\n')\n",
    "    print(\"SUCCESS: Schemas and row counts for all dataframes have been written to all_data_schemas.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading File Path: ../sql_data\\admissions.csv\n",
      "Reading File Path: ../sql_data\\diagnoses_icd.csv\n",
      "Reading File Path: ../sql_data\\discharge.csv\n",
      "Reading File Path: ../sql_data\\discharge_detail.csv\n",
      "Reading File Path: ../sql_data\\drgcodes.csv\n",
      "Reading File Path: ../sql_data\\d_hcpcs.csv\n",
      "Reading File Path: ../sql_data\\d_icd_diagnoses.csv\n",
      "Reading File Path: ../sql_data\\d_icd_procedures.csv\n",
      "Reading File Path: ../sql_data\\d_labitems.csv\n",
      "Reading File Path: ../sql_data\\emar.csv\n",
      "Reading File Path: ../sql_data\\emar_detail.csv\n",
      "Reading File Path: ../sql_data\\hcpcsevents.csv\n",
      "Reading File Path: ../sql_data\\labevents.csv\n",
      "Reading File Path: ../sql_data\\microbiologyevents.csv\n",
      "Reading File Path: ../sql_data\\omr.csv\n",
      "Reading File Path: ../sql_data\\patients.csv\n",
      "Reading File Path: ../sql_data\\pharmacy.csv\n",
      "Reading File Path: ../sql_data\\prescriptions.csv\n",
      "Reading File Path: ../sql_data\\procedures_icd.csv\n",
      "Reading File Path: ../sql_data\\radiology.csv\n",
      "Reading File Path: ../sql_data\\radiology_detail.csv\n",
      "Reading File Path: ../sql_data\\services.csv\n",
      "SUCCESS: All .csv Data in Data Directory Read\n"
     ]
    }
   ],
   "source": [
    "dataframes_list = read_all_files_in_data_dir('../sql_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Schemas and row counts for all dataframes have been written to all_data_schemas.txt\n"
     ]
    }
   ],
   "source": [
    "print_schema_for_all_dataframes(dataframes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
