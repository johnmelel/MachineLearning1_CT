{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Notes\n",
    "\n",
    "On macbook pro\n",
    "- Using arm64 architecture is faster than x86_64 for pyspark jobs: `arch -arm64 /bin/zsh`\n",
    "  - Verify: `uname -m`\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def start_spark():\n",
    "    spark = (SparkSession.builder \n",
    "        .appName(\"OptimizedPySpark\") \n",
    "        .master(\"local\") \n",
    "        .config(\"spark.driver.memory\", \"8g\") \n",
    "        .config(\"spark.executor.memory\", \"6g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\") \n",
    "        .config(\"spark.sql.shuffle.partitions\", \"100\")  \n",
    "        .config(\"spark.default.parallelism\", \"8\") \n",
    "        .config(\"spark.local.dir\", \"/tmp/spark-temp\")\n",
    "        .config(\"spark.rdd.compress\", \"true\") \n",
    "        .config(\"spark.memory.fraction\", \"0.8\") \n",
    "        .config(\"spark.memory.storageFraction\", \"0.3\") \n",
    "        .getOrCreate())\n",
    "\n",
    "    # Reduce shuffle partitions\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "\n",
    "    print(\"Spark Version:\", spark.version)\n",
    "    print(\"Spark UI: http://localhost:4040\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_resources(spark):\n",
    "    spark.stop()\n",
    "    return start_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/06 14:49:49 WARN Utils: Your hostname, Saganas-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.4.69 instead (on interface en0)\n",
      "25/03/06 14:49:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/06 14:49:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/06 14:49:49 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.4\n",
      "Spark UI: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "spark = start_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Drive is mounted successfully!\n",
      "Files in Drive: ['(ReferHere)Final_Dataset_Data_Folder ', 'merged_5000_patient_radio.csv', 'mimic-iv-ext-clinical-decision-making-a-mimic-iv-derived-dataset-for-evaluation-of-large-language-models-on-the-task-of-clinical-decision-making-for-abdominal-pathologies-1.1.zip', '.DS_Store', 'extracted_zip', 'JM outputs', 'SQL DB Export', 'mimiciv.db', 'mimic-iv-3.1.zip', 'Machine Learning I Team 5 Project Proposal.gdoc', 'YY_codes', 'mimic-iv-note-deidentified-free-text-clinical-notes-2.2.zip', 'merged_5000_patient.csv', 'Project Idea.gdoc', 'Final_Dataset_Data_Folder_unzip', 'MLI_2025_Winter', 'Sagana Outputs', 'merged_5000_patient_radio_disc.csv', 'Project Milestone-I.gdoc', 'Dataset Readme.gdoc']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "GOOGLE_DRIVE_LOCAL_MOUNT='/Users/sagana/Library/CloudStorage/GoogleDrive-sondande@uchicago.edu/.shortcut-targets-by-id/1O2pwlZERv3B7ki78Wn0brrpnArRBTFdH/MLI_2025 Winter/'\n",
    "\n",
    "# Check if Google Drive is accessible\n",
    "if os.path.exists(GOOGLE_DRIVE_LOCAL_MOUNT):\n",
    "    print(\"Google Drive is mounted successfully!\")\n",
    "    print(\"Files in Drive:\", os.listdir(GOOGLE_DRIVE_LOCAL_MOUNT))\n",
    "else:\n",
    "    print(\"Google Drive is not mounted. Please check your installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|           table|              schema|\n",
      "+----------------+--------------------+\n",
      "|   diagnoses_icd|['subject_id', 'h...|\n",
      "|       discharge|['subject_id', 'h...|\n",
      "|        drgcodes|['subject_id', 'h...|\n",
      "| d_icd_diagnoses|['icd_code', 'icd...|\n",
      "|d_icd_procedures|['icd_code', 'icd...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import collect_set, collect_list, struct, col, when, count, countDistinct, lit\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Read in schema file and process to get schemas needed\n",
    "schemas_df = spark.read.csv(f'{GOOGLE_DRIVE_LOCAL_MOUNT}/SQL DB Export/CSV/schema.csv', header=True)\n",
    "schemas_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------------+--------------------------------------------------------------------------------+\n",
      "|subject_id| hadm_id|          charttime|                                                                            text|\n",
      "+----------+--------+-------------------+--------------------------------------------------------------------------------+\n",
      "|  10000117|    NULL|2175-05-10 10:12:00|BILATERAL DIGITAL SCREENING MAMMOGRAM WITH CAD\\\\n\\\\nHISTORY:  Baseline screen...|\n",
      "|  10000117|    NULL|2177-05-23 13:18:00|INDICATION:  ___ female with right epigastric pain radiating to back,\\\\nrule ...|\n",
      "|  10000117|    NULL|2178-08-29 13:39:00|CLINICAL HISTORY:  Right upper quadrant pain, evaluate for gallstones.\\\\n\\\\nA...|\n",
      "|  10000117|22927623|2181-11-15 00:40:00|EXAMINATION:   CHEST (PA AND LAT)\\\\n\\\\nINDICATION:  History: ___ with PMH GER...|\n",
      "|  10000117|22927623|2181-11-15 00:47:00|EXAMINATION:   NECK SOFT TISSUES\\\\n\\\\nINDICATION:  ___ woman with dysphasia. ...|\n",
      "|  10000117|    NULL|2182-05-21 16:39:00|EXAMINATION:  LIVER OR GALLBLADDER US (SINGLE ORGAN)\\\\n\\\\nINDICATION:  ___ wi...|\n",
      "|  10000117|    NULL|2182-12-22 08:13:00|EXAMINATION:  LIVER OR GALLBLADDER US (SINGLE ORGAN)\\\\n\\\\nINDICATION:  ___ ye...|\n",
      "|  10000117|    NULL|2183-07-24 13:57:00|EXAMINATION:  CT HEAD W/O CONTRAST Q111 CT HEAD\\\\n\\\\nINDICATION:  ___ year ol...|\n",
      "|  10000117|27988844|2183-09-18 09:48:00|EXAMINATION:  Left hip radiographs, two views, and pelvis radiograph, single\\...|\n",
      "|  10000117|27988844|2183-09-18 12:29:00|EXAMINATION:  Chest radiographs, PA and lateral.\\\\n\\\\nINDICATION:  Preoperati...|\n",
      "|  10000117|27988844|2183-09-19 09:38:00|EXAMINATION:  HIP NAILING IN OR W/FILMS AND FLUORO LEFT\\\\n\\\\nINDICATION:  LEF...|\n",
      "|  10000117|    NULL|2183-10-03 08:26:00|INDICATION:  ___ year old woman with L hip fx// assess fx\\\\n\\\\nCOMPARISON:  R...|\n",
      "|  10000117|    NULL|2183-11-15 10:19:00|EXAMINATION:  HIP UNILAT MIN 2 VIEWS LEFT\\\\n\\\\nINDICATION:  ___ year old woma...|\n",
      "|  10000117|    NULL|2184-01-10 09:32:00|EXAMINATION:  HIP UNILAT MIN 2 VIEWS LEFT\\\\n\\\\nINDICATION:  ___ year old woma...|\n",
      "|  10000117|    NULL|2184-04-17 09:52:00|EXAMINATION:  HIP UNILAT MIN 2 VIEWS IN O.R. LEFT\\\\n\\\\nINDICATION:  ___ year ...|\n",
      "|  10000117|    NULL|2184-08-11 08:28:00|EXAMINATION:  HIP UNILAT MIN 2 VIEWS LEFT\\\\n\\\\nINDICATION:  ___ year old woma...|\n",
      "|  10000117|    NULL|2174-08-14 17:29:00|CHEST RADIOGRAPH PERFORMED.\\\\n\\\\nCOMPARISON:  None.\\\\n\\\\nCLINICAL HISTORY:  _...|\n",
      "|  10000117|    NULL|2175-05-10 10:13:00|INDICATION:  ___ female with dysfunctional uterine bleeding.\\\\n\\\\nNo prior ex...|\n",
      "|  10000248|    NULL|2192-11-29 20:50:00|EXAMINATION:  CT OF THE ABDOMEN AND PELVIS\\\\n\\\\nINDICATION:  Expanding right ...|\n",
      "|  10000560|    NULL|2189-06-26 14:01:00|INDICATION:  ___ woman with lower abdominal pain, constant _____,\\\\nevaluate ...|\n",
      "+----------+--------+-------------------+--------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct schema\n",
    "radiology_schema_list = ast.literal_eval(schemas_df.filter(col(\"table\") == 'radiology').select(col(\"schema\")).collect()[0][0])\n",
    "radiology_schema = StructType([\n",
    "    StructField(x, StringType(), True) for x in radiology_schema_list\n",
    "])\n",
    "\n",
    "# Read in radiology dataset\n",
    "radiology_df = spark.read.option(\"delimiter\", \"|\").option(\"quote\", '\"').option(\"multiLine\", \"true\").csv(f'{GOOGLE_DRIVE_LOCAL_MOUNT}/Sagana Outputs/Clinical Notes Creation/Input Data/radiology.csv', schema=radiology_schema)\n",
    "radiology_df.show(truncate= 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------------+--------------------------------------------------------------------------------+\n",
      "|subject_id| hadm_id|          charttime|                                                                            text|\n",
      "+----------+--------+-------------------+--------------------------------------------------------------------------------+\n",
      "|  10000117|27988844|2183-09-21 00:00:00| \\\\nName:  ___                 Unit No:   ___\\\\n \\\\nAdmission Date:  ___     ...|\n",
      "|  10000117|22927623|2181-11-15 00:00:00| \\\\nName:  ___                 Unit No:   ___\\\\n \\\\nAdmission Date:  ___     ...|\n",
      "|  10000248|20600184|2192-11-30 00:00:00| \\\\nName:  ___                      Unit No:   ___\\\\n \\\\nAdmission Date:  ___...|\n",
      "|  10000560|28979390|2189-10-17 00:00:00| \\\\nName:  ___                     Unit No:   ___\\\\n \\\\nAdmission Date:  ___ ...|\n",
      "|  10000764|27897940|2132-10-19 00:00:00| \\\\nName:  ___               Unit No:   ___\\\\n \\\\nAdmission Date:  ___       ...|\n",
      "|  10000826|28289260|2147-01-02 00:00:00| \\\\nName:  ___.          Unit No:   ___\\\\n \\\\nAdmission Date:  ___           ...|\n",
      "|  10000826|21086876|2146-12-24 00:00:00| \\\\nName:  ___.          Unit No:   ___\\\\n \\\\nAdmission Date:  ___           ...|\n",
      "|  10000826|20032235|2146-12-12 00:00:00| \\\\nName:  ___.          Unit No:   ___\\\\n \\\\nAdmission Date:  ___           ...|\n",
      "|  10011449|27619916|2135-12-01 00:00:00| \\\\nName:  ___                 Unit No:   ___\\\\n \\\\nAdmission Date:  ___     ...|\n",
      "|  10016367|23401924|2137-12-12 00:00:00| \\\\nName:  ___                    Unit No:   ___\\\\n \\\\nAdmission Date:  ___  ...|\n",
      "|  10016367|23401924|2137-12-12 00:00:00| \\\\nName:  ___                    Unit No:   ___\\\\n \\\\nAdmission Date:  ___  ...|\n",
      "|  10016367|26107656|2135-04-02 00:00:00| \\\\nName:  ___                    Unit No:   ___\\\\n \\\\nAdmission Date:  ___  ...|\n",
      "|  10016367|26107656|2135-04-02 00:00:00| \\\\nName:  ___                    Unit No:   ___\\\\n \\\\nAdmission Date:  ___  ...|\n",
      "|  10016367|20343876|2129-03-31 00:00:00| \\\\nName:  ___                    Unit No:   ___\\\\n \\\\nAdmission Date:  ___  ...|\n",
      "|  10016367|20343876|2129-03-31 00:00:00| \\\\nName:  ___                    Unit No:   ___\\\\n \\\\nAdmission Date:  ___  ...|\n",
      "|  10016367|27955224|2129-02-18 00:00:00| \\\\nName:  ___                    Unit No:   ___\\\\n \\\\nAdmission Date:  ___  ...|\n",
      "|  10016367|27955224|2129-02-18 00:00:00| \\\\nName:  ___                    Unit No:   ___\\\\n \\\\nAdmission Date:  ___  ...|\n",
      "|  10029484|20764029|2160-11-11 00:00:00| \\\\nName:  ___                Unit No:   ___\\\\n \\\\nAdmission Date:  ___      ...|\n",
      "|  10054496|25245648|2124-12-11 00:00:00| \\\\nName:  ___              Unit No:   ___\\\\n \\\\nAdmission Date:  ___        ...|\n",
      "|  10054496|25035451|2120-05-03 00:00:00| \\\\nName:  ___              Unit No:   ___\\\\n \\\\nAdmission Date:  ___        ...|\n",
      "+----------+--------+-------------------+--------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in radiology dataset\n",
    "discharge_schema_list = ast.literal_eval(schemas_df.filter(col(\"table\") == 'discharge').select(col(\"schema\")).collect()[0][0])\n",
    "discharge_schema = StructType([\n",
    "    StructField(x, StringType(), True) for x in discharge_schema_list\n",
    "])\n",
    "\n",
    "discharge_df = spark.read.option(\"delimiter\", \"|\").option(\"quote\", '\"').option(\"multiLine\", \"true\").csv(f'{GOOGLE_DRIVE_LOCAL_MOUNT}/Sagana Outputs/Clinical Notes Creation/Input Data/discharge.csv', schema=discharge_schema)\n",
    "discharge_df.show(truncate= 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only required fields\n",
    "radiology_df_filtered = radiology_df.select('subject_id', 'text')\n",
    "discharge_df_filtered = discharge_df.select('subject_id', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----------+-----------+---------+--------+--------------+-----+-----------------------+------------------------+----+------+------+----+\n",
      "|subject_id|gender|anchor_age|anchor_year|insurance|language|marital_status| race|blood_pressure_systolic|blood_pressure_diastolic| bmi|height|weight|egfr|\n",
      "+----------+------+----------+-----------+---------+--------+--------------+-----+-----------------------+------------------------+----+------+------+----+\n",
      "|  10000117|     F|        48|       2174| Medicaid| English|      DIVORCED|WHITE|                    108|                      74|18.9|    64|   110|NULL|\n",
      "|  10000161|     M|        60|       2163| Medicaid| English|        SINGLE|WHITE|                    106|                      92|NULL|  NULL|  NULL|NULL|\n",
      "|  10000248|     M|        34|       2192|  Private| English|       MARRIED|WHITE|                   NULL|                    NULL|25.5|    68|   168|NULL|\n",
      "|  10000280|     M|        20|       2151|  Private| English|          NULL|OTHER|                    125|                      77|NULL|  NULL| 170.5|NULL|\n",
      "|  10000560|     F|        53|       2189|  Private| English|       MARRIED|WHITE|                    124|                      78|NULL|  NULL|   128|NULL|\n",
      "+----------+------+----------+-----------+---------+--------+--------------+-----+-----------------------+------------------------+----+------+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter for only notes where we have a patient to ensure we filter down datasets\n",
    "patients_df = spark.read.csv(f'{GOOGLE_DRIVE_LOCAL_MOUNT}/JM outputs/patients_cleaned.csv', header=True)\n",
    "patients_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_radiology_df = radiology_df_filtered.join(patients_df, radiology_df_filtered.subject_id == patients_df.subject_id, 'left_semi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discharge_df = discharge_df_filtered.join(patients_df, discharge_df_filtered.subject_id == patients_df.subject_id, 'left_semi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_radiology_df.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet('radiology_filtered/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_discharge_df.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet('discharge_filtered/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Processed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_processed_df = spark.read.parquet('discharge_processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_processed_df = spark.read.parquet('radiology_processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import to_json, col\n",
    "\n",
    "df = discharge_processed_df.withColumn(\"sections\", to_json(col(\"sections\")))  # Convert map column to JSON string\n",
    "df = df.withColumn(\"entities\", to_json(col(\"entities\")))\n",
    "# df.write.csv(\"output_directory\", header=True, mode=\"overwrite\")\n",
    "# df.coalesce(1).write.csv(\"discharge_processed_csv/\", header=True, mode=\"overwrite\")\n",
    "df_PD = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PD.to_csv('discharge_processed_csv/discharge_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import to_json, col\n",
    "\n",
    "df = radio_processed_df.withColumn(\"sections\", to_json(col(\"sections\")))  # Convert map column to JSON string\n",
    "df = df.withColumn(\"entities\", to_json(col(\"entities\")))\n",
    "# df.write.csv(\"output_directory\", header=True, mode=\"overwrite\")\n",
    "# df.coalesce(1).write.csv(\"discharge_processed_csv/\", header=True, mode=\"overwrite\")\n",
    "df_PD = df.toPandas()\n",
    "df_PD.to_csv('radiology_processed_csv/radio_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>sections</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000117</td>\n",
       "      <td>BILATERAL DIGITAL SCREENING MAMMOGRAM WITH CAD...</td>\n",
       "      <td>BILATERAL DIGITAL SCREENING MAMMOGRAM WITH CAD...</td>\n",
       "      <td>{\"bilateral_digital_screening_mammogram_with_c...</td>\n",
       "      <td>{\"birads\":[\"BI-RADS 1\",\"bi-rads 1\",\"bi-rads\"],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000117</td>\n",
       "      <td>INDICATION:  ___ female with right epigastric ...</td>\n",
       "      <td>INDICATION: ___ female with right epigastric p...</td>\n",
       "      <td>{\"impression\":\"1. No gallstones and no signs o...</td>\n",
       "      <td>{\"birads\":[],\"locations\":[],\"procedures\":[],\"f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000117</td>\n",
       "      <td>CLINICAL HISTORY:  Right upper quadrant pain, ...</td>\n",
       "      <td>CLINICAL HISTORY: Right upper quadrant pain, e...</td>\n",
       "      <td>{\"impression\":\"Normal gallbladder. No gallston...</td>\n",
       "      <td>{\"birads\":[],\"locations\":[\"quadrant\"],\"procedu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000117</td>\n",
       "      <td>EXAMINATION:   CHEST (PA AND LAT)\\\\n\\\\nINDICAT...</td>\n",
       "      <td>EXAMINATION: CHEST (PA AND LAT) INDICATION: Hi...</td>\n",
       "      <td>{\"comparison\":\"Chest radiograph from ___.\",\"ex...</td>\n",
       "      <td>{\"birads\":[],\"locations\":[],\"procedures\":[],\"f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000117</td>\n",
       "      <td>EXAMINATION:   NECK SOFT TISSUES\\\\n\\\\nINDICATI...</td>\n",
       "      <td>EXAMINATION: NECK SOFT TISSUES INDICATION: ___...</td>\n",
       "      <td>{\"comparison\":\"None available.\",\"examination\":...</td>\n",
       "      <td>{\"birads\":[],\"locations\":[],\"procedures\":[],\"f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id                                               text  \\\n",
       "0   10000117  BILATERAL DIGITAL SCREENING MAMMOGRAM WITH CAD...   \n",
       "1   10000117  INDICATION:  ___ female with right epigastric ...   \n",
       "2   10000117  CLINICAL HISTORY:  Right upper quadrant pain, ...   \n",
       "3   10000117  EXAMINATION:   CHEST (PA AND LAT)\\\\n\\\\nINDICAT...   \n",
       "4   10000117  EXAMINATION:   NECK SOFT TISSUES\\\\n\\\\nINDICATI...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  BILATERAL DIGITAL SCREENING MAMMOGRAM WITH CAD...   \n",
       "1  INDICATION: ___ female with right epigastric p...   \n",
       "2  CLINICAL HISTORY: Right upper quadrant pain, e...   \n",
       "3  EXAMINATION: CHEST (PA AND LAT) INDICATION: Hi...   \n",
       "4  EXAMINATION: NECK SOFT TISSUES INDICATION: ___...   \n",
       "\n",
       "                                            sections  \\\n",
       "0  {\"bilateral_digital_screening_mammogram_with_c...   \n",
       "1  {\"impression\":\"1. No gallstones and no signs o...   \n",
       "2  {\"impression\":\"Normal gallbladder. No gallston...   \n",
       "3  {\"comparison\":\"Chest radiograph from ___.\",\"ex...   \n",
       "4  {\"comparison\":\"None available.\",\"examination\":...   \n",
       "\n",
       "                                            entities  \n",
       "0  {\"birads\":[\"BI-RADS 1\",\"bi-rads 1\",\"bi-rads\"],...  \n",
       "1  {\"birads\":[],\"locations\":[],\"procedures\":[],\"f...  \n",
       "2  {\"birads\":[],\"locations\":[\"quadrant\"],\"procedu...  \n",
       "3  {\"birads\":[],\"locations\":[],\"procedures\":[],\"f...  \n",
       "4  {\"birads\":[],\"locations\":[],\"procedures\":[],\"f...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_PD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
