{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Notes\n",
    "\n",
    "On macbook pro\n",
    "- Using arm64 architecture is faster than x86_64 for pyspark jobs: `arch -arm64 /bin/zsh`\n",
    "  - Verify: `uname -m`\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def start_spark():\n",
    "    spark = (SparkSession.builder\n",
    "             .master(\"local[*]\") # use all cores on computer/dynamically adjust based of cpu count and maximize parallism forcomput vector opertations\n",
    "             .appName(\"Clinical_Notes_Processing\")\n",
    "             .config(\"spark.driver.memory\", \"8g\") # half of ram on mac\n",
    "             .config(\"spark.executor.memory\", \"4g\")\n",
    "             .config(\"spark.driver.maxResultSize\", \"2g\") # increase for embeddings rapidly growing\n",
    "             .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "             .config(\"spark.executor.cores\", \"4\")\n",
    "             .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") # more efficient garbage collector. Helpful for large heaps containing embedded vectors \n",
    "             .getOrCreate())\n",
    "\n",
    "    # Reduce shuffle partitions\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "\n",
    "    print(\"Spark Version:\", spark.version)\n",
    "    print(\"Spark UI: http://localhost:4040\")\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_resources(spark):\n",
    "    spark.stop()\n",
    "    return start_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = start_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GOOGLE_DRIVE_LOCAL_MOUNT='/Users/sagana/Library/CloudStorage/GoogleDrive-sondande@uchicago.edu/.shortcut-targets-by-id/1O2pwlZERv3B7ki78Wn0brrpnArRBTFdH/MLI_2025 Winter/'\n",
    "\n",
    "# Check if Google Drive is accessible\n",
    "if os.path.exists(GOOGLE_DRIVE_LOCAL_MOUNT):\n",
    "    print(\"Google Drive is mounted successfully!\")\n",
    "    print(\"Files in Drive:\", os.listdir(GOOGLE_DRIVE_LOCAL_MOUNT))\n",
    "else:\n",
    "    print(\"Google Drive is not mounted. Please check your installation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import collect_set, collect_list, struct, col, when, count, countDistinct, lit\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Read in schema file and process to get schemas needed\n",
    "schemas_df = spark.read.csv(f'{GOOGLE_DRIVE_LOCAL_MOUNT}/SQL DB Export/CSV/schema.csv', header=True)\n",
    "schemas_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct schema\n",
    "radiology_schema_list = ast.literal_eval(schemas_df.filter(col(\"table\") == 'radiology').select(col(\"schema\")).collect()[0][0])\n",
    "radiology_schema = StructType([\n",
    "    StructField(x, StringType(), True) for x in radiology_schema_list\n",
    "])\n",
    "\n",
    "# Read in radiology dataset\n",
    "radiology_df = spark.read.option(\"delimiter\", \"|\").option(\"quote\", '\"').option(\"multiLine\", \"true\").csv(f'{GOOGLE_DRIVE_LOCAL_MOUNT}/Sagana Outputs/Clinical Notes Creation/Input Data/radiology.csv', schema=radiology_schema)\n",
    "radiology_df.show(truncate= 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in radiology dataset\n",
    "discharge_schema_list = ast.literal_eval(schemas_df.filter(col(\"table\") == 'discharge').select(col(\"schema\")).collect()[0][0])\n",
    "discharge_schema = StructType([\n",
    "    StructField(x, StringType(), True) for x in discharge_schema_list\n",
    "])\n",
    "\n",
    "discharge_df = spark.read.option(\"delimiter\", \"|\").option(\"quote\", '\"').option(\"multiLine\", \"true\").csv(f'{GOOGLE_DRIVE_LOCAL_MOUNT}/Sagana Outputs/Clinical Notes Creation/Input Data/discharge.csv', schema=discharge_schema)\n",
    "discharge_df.show(truncate= 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only required fields\n",
    "radiology_df_filtered = radiology_df.select('subject_id', 'text')\n",
    "discharge_df_filtered = discharge_df.select('subject_id', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for only notes where we have a patient to ensure we filter down datasets\n",
    "patients_df = spark.read.csv(f'{GOOGLE_DRIVE_LOCAL_MOUNT}/JM outputs/patients_cleaned.csv', header=True)\n",
    "patients_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_radiology_df = radiology_df_filtered.join(patients_df, radiology_df_filtered.subject_id == patients_df.subject_id, 'left_semi')\n",
    "final_radiology_df_updated = final_radiology_df.withColumnRenamed('text', 'radiology_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_discharge_df = discharge_df_filtered.join(patients_df, discharge_df_filtered.subject_id == patients_df.subject_id, 'left_semi')\n",
    "final_discharge_df_updated = final_discharge_df.withColumnRenamed('text', 'discharge_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = final_radiology_df_updated.join(final_discharge_df_updated, how='inner', on=['subject_id'])\n",
    "combined_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the dataset for sample set for embeddings\n",
    "# Sample without replacement to have a dataset that is representative of the original dataset\n",
    "# Add seed for reproducibility\n",
    "\n",
    "# Ensure sampling contains same subject_ids in both datasets\n",
    "combined_df_sample = combined_df.sample(False, 0.002, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df_sample.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet('combined_cn_sample/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_radiology_df_count  = final_radiology_df.count()\n",
    "# distinct_radiology_df_count = final_radiology_df.select('subject_id').distinct().count()\n",
    "# final_discharge_df_count = final_discharge_df.count()\n",
    "# distinct_discharge_df_count = final_discharge_df.select('subject_id').distinct().count()\n",
    "\n",
    "# sampled_rad_count = sampled_final_radiology_df.count()\n",
    "# distinct_rad_count = sampled_final_radiology_df.select('subject_id').distinct().count()\n",
    "# sampled_dis_count = sampled_final_discharge_df.count()\n",
    "# distinct_dis_count = sampled_final_discharge_df.select('subject_id').distinct().count()\n",
    "\n",
    "# print(f\"Original Radiology Count: {final_radiology_df_count}, Distinct Radiology Count: {distinct_radiology_df_count}\")\n",
    "# print(f\"Sampled Radiology Count: {sampled_rad_count}, Distinct Radiology Count: {distinct_rad_count}\")\n",
    "# print(f\"Original Discharge Count: {final_discharge_df_count}, Distinct Discharge Count: {distinct_discharge_df_count}\")\n",
    "# print(f\"Sampled Discharge Count: {sampled_dis_count}, Distinct Discharge Count: {distinct_dis_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Suppose \"sampled_df\" has a vector column \"embeddings\"\n",
    "# # 1. Convert Spark vector to array, then collect\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.ml.linalg import VectorUDT\n",
    "# import numpy as np\n",
    "\n",
    "# # Convert Spark Vector to Python list\n",
    "# def to_array(v):\n",
    "#     return v.toArray().tolist()\n",
    "\n",
    "# to_array_udf = udf(to_array, \"array<double>\")\n",
    "# sampled_df_array = sampled_df.withColumn(\"embeddings_array\", to_array_udf(\"embeddings\"))\n",
    "\n",
    "# # 2. Collect to Pandas\n",
    "# pdf = sampled_df_array.select(\"embeddings_array\").limit(5000).toPandas()  # limit for memory safety\n",
    "# X = np.array(pdf[\"embeddings_array\"].tolist())  # shape: (n_samples, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_radiology_df.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet('radiology_filtered/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_discharge_df.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet('discharge_filtered/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Processed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discharge_processed_df = spark.read.parquet('discharge_processed/')\n",
    "# radio_processed_df = spark.read.parquet('radiology_processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import expr\n",
    "# from pyspark.sql.functions import to_json, cola\n",
    "\n",
    "# df = discharge_processed_df.withColumn(\"sections\", to_json(col(\"sections\")))  # Convert map column to JSON string\n",
    "# df = df.withColumn(\"entities\", to_json(col(\"entities\")))\n",
    "# # df.write.csv(\"output_directory\", header=True, mode=\"overwrite\")\n",
    "# # df.coalesce(1).write.csv(\"discharge_processed_csv/\", header=True, mode=\"overwrite\")\n",
    "# df_PD = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_PD.to_csv('discharge_processed_csv/discharge_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import expr\n",
    "# from pyspark.sql.functions import to_json, col\n",
    "\n",
    "# df = radio_processed_df.withColumn(\"sections\", to_json(col(\"sections\")))  # Convert map column to JSON string\n",
    "# df = df.withColumn(\"entities\", to_json(col(\"entities\")))\n",
    "# # df.write.csv(\"output_directory\", header=True, mode=\"overwrite\")\n",
    "# # df.coalesce(1).write.csv(\"discharge_processed_csv/\", header=True, mode=\"overwrite\")\n",
    "# df_PD = df.toPandas()\n",
    "# df_PD.to_csv('radiology_processed_csv/radio_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_PD.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review embeddings output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = start_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_dis = spark.read.parquet('discharge_text/clinical_notes_sampled_embedded/')\n",
    "embedded_radio = spark.read.parquet('radiology_text/clinical_notes_sampled_embedded/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_dis.show(1, truncate=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_radio.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_radio_emb = embedded_radio.select('subject_id', 'embedding_radiology_text')\n",
    "filtered_dis_emb = embedded_dis.select('subject_id', 'embedding_discharge_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_radio_emb.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_radio_agg = (\n",
    "    filtered_radio_emb\n",
    "    .groupBy(\"subject_id\")\n",
    "    .agg(\n",
    "        collect_set(\"icd_code\").alias(\"proc_codes\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_notes_combined_em = filtered_radio_emb.join(filtered_dis_emb, how='inner', on=['subject_id'])\n",
    "clinical_notes_combined_em.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_notes_combined_em.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_notes_PD = clinical_notes_combined_em.toPandas()\n",
    "# c_notes_PD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_notes_PD.to_csv('embedded_clinical_notes_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_notes_PD.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clinical_notes_combined_em.write.mode(\"overwrite\").option(\"compression\", \"snappy\").parquet('embedded_clinical_notes_combined/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
